<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>SVM用于分类任务</title>
    <link href="/2023/05/08/SVM%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/"/>
    <url>/2023/05/08/SVM%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="自实现核SVM用于分类任务"><a href="#自实现核SVM用于分类任务" class="headerlink" title="自实现核SVM用于分类任务"></a>自实现核SVM用于分类任务</h1><p>支持向量机是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。</p><p>在这里，考虑基于核方法的支持向量机，并使用SMO算法去训练它，这是LIBSVM和sklearn中训练核SVM的方法，但在project1中我自行实现SMO算法求解器。</p><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>SMO（Sequential Minimal Optimization）是求解SVM问题的高效算法之一，libSVM采用的正是该算法。SMO算法其实是一种启发式算法：先选择两个变量 $α_i$ 和 $α_j$ ，然后固定其他参数，从而将问题转化成一个二变量的二次规划问题。求出能使目标最大的一对 $α_i$ 和$α_j$ 后，将它们固定，再选择两个变量，直到目标值收敛。</p><p>采用Chang, Chih-Chung, and Chih-Jen Lin. “LIBSVM: a library for support vector machines.” ACM transactions on intelligent systems and technology (TIST) 2.3 (2011): 1-27.中的实现方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> lru_cache<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solver</span>:<br>    <span class="hljs-string">r&#x27;&#x27;&#x27;SMO算法求解器，迭代求解下面的问题:</span><br><span class="hljs-string">    .. math:: \min_&#123;\pmb\alpha&#125;\quad&amp;\frac12\pmb\alpha^T\pmb Q\pmb\alpha+\pmb p^T\pmb\alpha\\</span><br><span class="hljs-string">        \text&#123;s.t.&#125;\quad&amp;\pmb y^T\pmb\alpha=0\\</span><br><span class="hljs-string">        &amp;0\leq\alpha_i\leq C,i=1,\cdots,l</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    Q : numpy.ndarray</span><br><span class="hljs-string">        优化问题中的 :math:`\pmb Q` 矩阵；</span><br><span class="hljs-string">    p : numpy.ndarray</span><br><span class="hljs-string">        优化问题中的 :math:`\pmb p` 向量；</span><br><span class="hljs-string">    y : numpy.ndarray</span><br><span class="hljs-string">        优化问题中的 :math:`\pmb y` 向量；</span><br><span class="hljs-string">    C : float</span><br><span class="hljs-string">        优化问题中的 :math:`C` 变量；</span><br><span class="hljs-string">    tol : float, default=1e-5</span><br><span class="hljs-string">        变量选择的tolerance，默认为1e-5.</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 Q: np.ndarray,</span><br><span class="hljs-params">                 p: np.ndarray,</span><br><span class="hljs-params">                 y: np.ndarray,</span><br><span class="hljs-params">                 C: <span class="hljs-built_in">float</span>,</span><br><span class="hljs-params">                 tol: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        problem_size = p.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">assert</span> problem_size == y.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> Q <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> problem_size == Q.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">assert</span> problem_size == Q.shape[<span class="hljs-number">1</span>]<br><br>        self.Q = Q<br>        self.p = p<br>        self.y = y<br>        self.C = C<br>        self.tol = tol<br>        self.alpha = np.zeros(problem_size)<br><br>        <span class="hljs-comment"># Calculate -y·▽f(α)</span><br>        self.neg_y_grad = -y * p<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">working_set_select</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">r&#x27;&#x27;&#x27;工作集选择，这里采用一阶选择:</span><br><span class="hljs-string">        .. math:: \pmb&#123;I&#125;_&#123;up&#125;(\pmb\alpha)&amp;=\&#123;t|\alpha_t&lt;C,y_t=1\text&#123; or &#125;\alpha_t&gt;0,y_t=-1\&#125;\\</span><br><span class="hljs-string">                 \pmb&#123;I&#125;_&#123;low&#125;(\pmb\alpha)&amp;=\&#123;t|\alpha_t&lt;C,y_t=-1\text&#123; or &#125;\alpha_t&gt;0,y_t=1\&#125;\\</span><br><span class="hljs-string">                 i&amp;\in\arg\max_&#123;t&#125;\&#123;-y_t\nabla_tf(\pmb\alpha)|t\in\pmb&#123;I&#125;_&#123;up&#125;(\pmb\alpha)\&#125;\\</span><br><span class="hljs-string">                 j&amp;\in\arg\max_&#123;t&#125;\&#123;-y_t\nabla_tf(\pmb\alpha)|t\in\pmb&#123;I&#125;_&#123;low&#125;(\pmb\alpha)\&#125;\\</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        Iup = np.argwhere(<br>            np.logical_or(<br>                np.logical_and(self.alpha &lt; self.C, self.y &gt; <span class="hljs-number">0</span>),<br>                np.logical_and(self.alpha &gt; <span class="hljs-number">0</span>, self.y &lt; <span class="hljs-number">0</span>),<br>            )).flatten()<br>        Ilow = np.argwhere(<br>            np.logical_or(<br>                np.logical_and(self.alpha &lt; self.C, self.y &lt; <span class="hljs-number">0</span>),<br>                np.logical_and(self.alpha &gt; <span class="hljs-number">0</span>, self.y &gt; <span class="hljs-number">0</span>),<br>            )).flatten()<br><br>        find_fail = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">try</span>:<br>            i = Iup[np.argmax(self.neg_y_grad[Iup])]<br>            j = Ilow[np.argmin(self.neg_y_grad[Ilow])]<br>        <span class="hljs-keyword">except</span>:<br>            find_fail = <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">if</span> find_fail <span class="hljs-keyword">or</span> self.neg_y_grad[i] - self.neg_y_grad[j] &lt; self.tol:<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> i, j<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, i: <span class="hljs-built_in">int</span>, j: <span class="hljs-built_in">int</span>, func=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;变量更新，在保证变量满足约束的条件下对两变量进行更新&#x27;&#x27;&#x27;</span><br>        Qi, Qj = self.get_Q(i, func), self.get_Q(j, func)<br>        yi, yj = self.y[i], self.y[j]<br>        alpha_i, alpha_j = self.alpha[i], self.alpha[j]<br><br>        quad_coef = Qi[i] + Qj[j] - <span class="hljs-number">2</span> * yi * yj * Qi[j]<br>        <span class="hljs-keyword">if</span> quad_coef &lt;= <span class="hljs-number">0</span>:<br>            quad_coef = <span class="hljs-number">1e-12</span><br><br>        <span class="hljs-keyword">if</span> yi * yj == -<span class="hljs-number">1</span>:<br>            delta = (self.neg_y_grad[i] * yi +<br>                     self.neg_y_grad[j] * yj) / quad_coef<br>            diff = alpha_i - alpha_j<br>            self.alpha[i] += delta<br>            self.alpha[j] += delta<br><br>            <span class="hljs-keyword">if</span> diff &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">if</span> (self.alpha[j] &lt; <span class="hljs-number">0</span>):<br>                    self.alpha[j] = <span class="hljs-number">0</span><br>                    self.alpha[i] = diff<br><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> (self.alpha[i] &lt; <span class="hljs-number">0</span>):<br>                    self.alpha[i] = <span class="hljs-number">0</span><br>                    self.alpha[j] = -diff<br><br>            <span class="hljs-keyword">if</span> diff &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">if</span> (self.alpha[i] &gt; self.C):<br>                    self.alpha[i] = self.C<br>                    self.alpha[j] = self.C - diff<br><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> (self.alpha[j] &gt; self.C):<br>                    self.alpha[j] = self.C<br>                    self.alpha[i] = self.C + diff<br><br>        <span class="hljs-keyword">else</span>:<br>            delta = (self.neg_y_grad[j] * yj -<br>                     self.neg_y_grad[i] * yi) / quad_coef<br>            <span class="hljs-built_in">sum</span> = self.alpha[i] + self.alpha[j]<br>            self.alpha[i] -= delta<br>            self.alpha[j] += delta<br><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span> &gt; self.C:<br>                <span class="hljs-keyword">if</span> self.alpha[i] &gt; self.C:<br>                    self.alpha[i] = self.C<br>                    self.alpha[j] = <span class="hljs-built_in">sum</span> - self.C<br><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> self.alpha[j] &lt; <span class="hljs-number">0</span>:<br>                    self.alpha[j] = <span class="hljs-number">0</span><br>                    self.alpha[i] = <span class="hljs-built_in">sum</span><br><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span> &gt; self.C:<br>                <span class="hljs-keyword">if</span> self.alpha[j] &gt; self.C:<br>                    self.alpha[j] = self.C<br>                    self.alpha[i] = <span class="hljs-built_in">sum</span> - self.C<br><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> self.alpha[i] &lt; <span class="hljs-number">0</span>:<br>                    self.alpha[i] = <span class="hljs-number">0</span><br>                    self.alpha[j] = <span class="hljs-built_in">sum</span><br><br>        delta_i = self.alpha[i] - alpha_i<br>        delta_j = self.alpha[j] - alpha_j<br>        self.neg_y_grad -= self.y * (delta_i * Qi + delta_j * Qj)<br>        <span class="hljs-keyword">return</span> delta_i, delta_j<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_rho</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">r&#x27;&#x27;&#x27;计算偏置项</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        .. math:: \rho=\dfrac&#123;\sum_&#123;i:0&lt;\alpha_i&lt;C&#125;y_i\nabla_if(\pmb\alpha)&#125;&#123;|\&#123;i\vert0&lt;\alpha_i&lt;C\&#125;|&#125;</span><br><span class="hljs-string">        如果不存在满足条件的支持向量，那么</span><br><span class="hljs-string">        .. math:: -M(\pmb\alpha)&amp;=\max\&#123;y_i\nabla_if(\pmb\alpha)|\alpha_i=0,y_i=-1\text&#123; or &#125;\alpha_i=C,y_i=1\&#125;\\</span><br><span class="hljs-string">                  -m(\pmb\alpha)&amp;=\max\&#123;y_i\nabla_if(\pmb\alpha)|\alpha_i=0,y_i=1\text&#123; or &#125;\alpha_i=C,y_i=-1\&#125;\\</span><br><span class="hljs-string">                  \rho&amp;=-\dfrac&#123;M(\pmb\alpha)+m(\pmb\alpha)&#125;&#123;2&#125;</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        sv = np.logical_and(<br>            self.alpha &gt; <span class="hljs-number">0</span>,<br>            self.alpha &lt; self.C,<br>        )<br>        <span class="hljs-keyword">if</span> sv.<span class="hljs-built_in">sum</span>() &gt; <span class="hljs-number">0</span>:<br>            rho = -np.average(self.neg_y_grad[sv])<br>        <span class="hljs-keyword">else</span>:<br>            ub_id = np.logical_or(<br>                np.logical_and(self.alpha == <span class="hljs-number">0</span>, self.y &lt; <span class="hljs-number">0</span>),<br>                np.logical_and(self.alpha == self.C, self.y &gt; <span class="hljs-number">0</span>),<br>            )<br>            lb_id = np.logical_or(<br>                np.logical_and(self.alpha == <span class="hljs-number">0</span>, self.y &gt; <span class="hljs-number">0</span>),<br>                np.logical_and(self.alpha == self.C, self.y &lt; <span class="hljs-number">0</span>),<br>            )<br>            rho = -(self.neg_y_grad[lb_id].<span class="hljs-built_in">min</span>() +<br>                    self.neg_y_grad[ub_id].<span class="hljs-built_in">max</span>()) / <span class="hljs-number">2</span><br>        <span class="hljs-keyword">return</span> rho<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_Q</span>(<span class="hljs-params">self, i: <span class="hljs-built_in">int</span>, func=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;获取核矩阵的第i行/列，即</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        .. math:: [K(\pmb x_1, \pmb x_i),\cdots,K(\pmb x_l, \pmb x_i)]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> self.Q[i]<br></code></pre></td></tr></table></figure><h2 id="SVM接口的实现"><a href="#SVM接口的实现" class="headerlink" title="SVM接口的实现"></a>SVM接口的实现</h2><p>为了让模型能够进行参数选择等功能，按照sklearn接口的模式编写SVM模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.base <span class="hljs-keyword">import</span> BaseEstimator<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> solver <span class="hljs-keyword">import</span> Solver, SolverWithCache<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BiLinearSVC</span>(<span class="hljs-title class_ inherited__">BaseEstimator</span>):<br>    <span class="hljs-string">r&#x27;&#x27;&#x27;二分类线性SVM，该类被多分类LinearSVC继承，所以不需要使用它。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    通过求解对偶问题</span><br><span class="hljs-string">    .. math:: \min_&#123;\pmb\alpha&#125;\quad&amp;\dfrac12\pmb\alpha^\top Q\pmb\alpha-\pmb&#123;e&#125;^\top\pmb&#123;\alpha&#125;\\</span><br><span class="hljs-string">        \text&#123;s.t.&#125;\quad&amp; \pmb&#123;y&#125;^\top\pmb\alpha=0,\\</span><br><span class="hljs-string">        &amp;0\leqslant\alpha_i\leqslant C,i=1,\cdots ,l</span><br><span class="hljs-string">    得到决策边界</span><br><span class="hljs-string">    .. math:: f(\pmb x)=\sum_&#123;i=1&#125;^ly_i\alpha_i\pmb x_i^T\pmb x-\rho</span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    C : float, default=1</span><br><span class="hljs-string">        SVM的正则化参数，默认为1；</span><br><span class="hljs-string">    max_iter : int, default=1000</span><br><span class="hljs-string">        SMO算法迭代次数，默认1000；</span><br><span class="hljs-string">    tol : float, default=1e-5</span><br><span class="hljs-string">        SMO算法的容忍度参数，默认1e-5；</span><br><span class="hljs-string">    cache_size : int, default=256</span><br><span class="hljs-string">        lru缓存大小，默认256，如果为0则不使用缓存，计算Q矩阵然后求解.</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 C: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.</span>,</span><br><span class="hljs-params">                 max_iter: <span class="hljs-built_in">int</span> = <span class="hljs-number">1000</span>,</span><br><span class="hljs-params">                 tol: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span>,</span><br><span class="hljs-params">                 cache_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">256</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.C = C<br>        self.max_iter = max_iter<br>        self.tol = tol<br>        self.cache_size = cache_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X: np.ndarray, y: np.ndarray</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;训练模型</span><br><span class="hljs-string">        Parameters</span><br><span class="hljs-string">        ----------</span><br><span class="hljs-string">        X : np.ndarray</span><br><span class="hljs-string">            训练集特征;</span><br><span class="hljs-string">        y : np.array</span><br><span class="hljs-string">            训练集标签，建议0为负标签，1为正标签.</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        X, y = np.array(X), np.array(y, dtype=<span class="hljs-built_in">float</span>)<br>        y[y != <span class="hljs-number">1</span>] = -<span class="hljs-number">1</span><br>        l, self.n_features = X.shape<br>        p = -np.ones(l)<br><br>        w = np.zeros(self.n_features)<br>        <span class="hljs-keyword">if</span> self.cache_size == <span class="hljs-number">0</span>:<br>            Q = y.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) * y * np.matmul(X, X.T)<br>            solver = Solver(Q, p, y, self.C, self.tol)<br>        <span class="hljs-keyword">else</span>:<br>            solver = SolverWithCache(p, y, self.C, self.tol, self.cache_size)<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">i</span>):<br>            <span class="hljs-keyword">return</span> y * np.matmul(X, X[i]) * y[i]<br><br>        <span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.max_iter):<br>            i, j = solver.working_set_select()<br>            <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br><br>            delta_i, delta_j = solver.update(i, j, func)<br>            w += delta_i * y[i] * X[i] + delta_j * y[j] * X[j]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;LinearSVC not coverage with &#123;&#125; iterations&quot;</span>.<span class="hljs-built_in">format</span>(<br>                self.max_iter))<br><br>        self.coef_ = (w, solver.calculate_rho())<br>        <span class="hljs-keyword">return</span> self<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decision_function</span>(<span class="hljs-params">self, X: np.ndarray</span>) -&gt; np.ndarray:<br>        <span class="hljs-string">&#x27;&#x27;&#x27;决策函数，输出预测值&#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> np.matmul(self.coef_[<span class="hljs-number">0</span>], np.array(X).T) - self.coef_[-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X: np.ndarray</span>) -&gt; np.ndarray:<br>        <span class="hljs-string">&#x27;&#x27;&#x27;预测函数，输出预测标签(0-1)&#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> (self.decision_function(np.array(X)) &gt;= <span class="hljs-number">0</span>).astype(<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">score</span>(<span class="hljs-params">self, X: np.ndarray, y: np.ndarray</span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-string">&#x27;&#x27;&#x27;评估函数，给定特征和标签，输出正确率&#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> accuracy_score(y, self.predict(X))<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>选用UCI数据集中的<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/">威斯康辛乳腺癌数据</a>作为实验数据集，首先查看数据分布等信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder<br><br><br><span class="hljs-comment"># 读取数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_wdbc</span>():<br>    feature_list, target_list = [], []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;data/wdbc.data&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(lines, desc=<span class="hljs-string">&#x27;reading data&#x27;</span>):<br>        line_split = line.split(<span class="hljs-string">&#x27;,&#x27;</span>)[<span class="hljs-number">1</span>:]<br>        feature_list.append(line_split[<span class="hljs-number">1</span>:])<br>        target_list.append(line_split[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">return</span> (<br>        np.array(feature_list).astype(<span class="hljs-built_in">float</span>),<br>        LabelEncoder().fit_transform(np.array(target_list)),<br>    )<br><br><br>X, y = read_wdbc()<br><span class="hljs-built_in">print</span>(X.shape, y.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">reading data: 100%|██████████████████████████████████████████████████████████████████████████| 569/569 [00:00&lt;?, ?it/s](569, 30) (569,)</code></pre><p>随机选10个特征观察相关性，注意到一些特征间存在很强的线性相关性，此外，很多特征的分布是近似正态分布，比较适合SVM模型分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>sns.pairplot(pd.DataFrame(X[:, np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>)]))<br></code></pre></td></tr></table></figure><pre><code class="hljs">&lt;seaborn.axisgrid.PairGrid at 0x1a6c91c4d90&gt;</code></pre><img src="/2023/05/08/SVM%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/output_8_1.png" class="" title="图片引用方法一"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler, StandardScaler<br><span class="hljs-keyword">from</span> svm <span class="hljs-keyword">import</span> BiLinearSVC, BiKernelSVC<br><br><span class="hljs-comment"># 训练集测试集划分</span><br>train_X, test_X, train_y, test_y = train_test_split(<br>    X,<br>    y,<br>    train_size=<span class="hljs-number">0.8</span>,<br>    random_state=<span class="hljs-number">42</span>,<br>)<br><br><span class="hljs-comment"># 数据归一化</span><br>stder = MinMaxScaler().fit(train_X)<br>train_X = stder.transform(train_X)<br>test_X = stder.transform(test_X)<br><br><span class="hljs-comment"># 线性核SVM和RBF核SVM</span><br>linear_model = BiLinearSVC().fit(train_X, train_y)<br>rbf_model = BiKernelSVC().fit(train_X, train_y)<br>acc1 = linear_model.score(test_X, test_y)<br>acc2 = rbf_model.score(test_X, test_y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;LinearSVM : &#123;:2.4f&#125;%, Kernel SVM : &#123;:2.4f&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(<br>    acc1 * <span class="hljs-number">100</span>, acc2 * <span class="hljs-number">100</span>))<br><br></code></pre></td></tr></table></figure><pre><code class="hljs">LinearSVM : 98.2456%, Kernel SVM : 96.4912%</code></pre><p>在Project1中，似乎线性分类器比核RBF效果更好。</p><h2 id="参数选择"><a href="#参数选择" class="headerlink" title="参数选择"></a>参数选择</h2><p>线性SVM只有一个超参数$C$，观察不同$C$情况下模型的表现，同时选出最优参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> warnings <span class="hljs-keyword">import</span> filterwarnings<br><br>filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br>C_list = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">50</span>)<br>n_iter = <span class="hljs-number">20</span><br>score_matrix = np.zeros((<span class="hljs-built_in">len</span>(C_list), n_iter))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(n_iter)):<br>    train_X, test_X, train_y, test_y = train_test_split(<br>        X,<br>        y,<br>        train_size=<span class="hljs-number">0.8</span>,<br>    )<br>    stder = MinMaxScaler().fit(train_X)<br>    train_X = stder.transform(train_X)<br>    test_X = stder.transform(test_X)<br>    <span class="hljs-keyword">for</span> j, C <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(C_list):<br>        score_matrix[j, i] = BiLinearSVC(C=C, max_iter=<span class="hljs-number">2500</span>).fit(<br>            train_X,<br>            train_y,<br>        ).score(<br>            test_X,<br>            test_y,<br>        )<br><br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>sns.<span class="hljs-built_in">set</span>()<br><br>mean_score = score_matrix.mean(<span class="hljs-number">1</span>)<br>plt.plot(C_list, mean_score, label=<span class="hljs-string">&#x27;Test accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;C&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;best C : &#123;&#125;, best accuracy : &#123;:2.4f&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(<br>    C_list[mean_score.argmax()],<br>    mean_score.<span class="hljs-built_in">max</span>() * <span class="hljs-number">100</span>,<br>))<br></code></pre></td></tr></table></figure><pre><code class="hljs">100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:46&lt;00:00,  2.32s/it]</code></pre><img src="/2023/05/08/SVM%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/output_11_1.png" class=""><pre><code class="hljs">best C : 3.3932217718953264, best accuracy : 97.4123%</code></pre><p>接下来看核SVM的表现，首先用不同的核函数进行实验，发现性能最优的是RBF。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python">kernel_list = [<span class="hljs-string">&quot;rbf&quot;</span>, <span class="hljs-string">&quot;poly&quot;</span>, <span class="hljs-string">&quot;sigmoid&quot;</span>]<br>n_iter = <span class="hljs-number">20</span><br><br>score_matrix = np.zeros((<span class="hljs-built_in">len</span>(kernel_list), n_iter))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(n_iter)):<br>    train_X, test_X, train_y, test_y = train_test_split(<br>        X,<br>        y,<br>        train_size=<span class="hljs-number">0.8</span>,<br>    )<br>    stder = MinMaxScaler().fit(train_X)<br>    train_X = stder.transform(train_X)<br>    test_X = stder.transform(test_X)<br>    <span class="hljs-keyword">for</span> j, kernel <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kernel_list):<br>        score_matrix[j, i] = BiKernelSVC(<br>            kernel=kernel,<br>            max_iter=<span class="hljs-number">5000</span>,<br>        ).fit(<br>            train_X,<br>            train_y,<br>        ).score(<br>            test_X,<br>            test_y,<br>        )<br><br><span class="hljs-keyword">for</span> i, kernel <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kernel_list):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;:7s&#125;, mean : &#123;:4.2f&#125;%, std : &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<br>        kernel,<br>        score_matrix[i].mean() * <span class="hljs-number">100</span>,<br>        (score_matrix[i] * <span class="hljs-number">100</span>).std(),<br>    ))<br><br></code></pre></td></tr></table></figure><pre><code class="hljs">100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 22.79it/s]rbf    , mean : 96.80%, std : 1.8664844588555454poly   , mean : 94.96%, std : 1.9392711376696599sigmoid, mean : 96.36%, std : 1.2490377952542226</code></pre><p>接下来为RBF选取最优参数$C,\gamma$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python">C_list = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br>G_list = np.logspace(-<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">20</span>)<br>n_iter = <span class="hljs-number">20</span><br>score_matrix = np.zeros((<span class="hljs-built_in">len</span>(C_list), <span class="hljs-built_in">len</span>(G_list), n_iter))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(n_iter)):<br>    train_X, test_X, train_y, test_y = train_test_split(<br>        X,<br>        y,<br>        train_size=<span class="hljs-number">0.8</span>,<br>    )<br>    stder = MinMaxScaler().fit(train_X)<br>    train_X = stder.transform(train_X)<br>    test_X = stder.transform(test_X)<br>    <span class="hljs-keyword">for</span> j, C <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(C_list):<br>        <span class="hljs-keyword">for</span> k, gamma <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(G_list):<br>            score_matrix[j, k, i] = BiKernelSVC(<br>                C=C,<br>                gamma=gamma,<br>                max_iter=<span class="hljs-number">2500</span>,<br>            ).fit(<br>                train_X,<br>                train_y,<br>            ).score(<br>                test_X,<br>                test_y,<br>            )<br></code></pre></td></tr></table></figure><pre><code class="hljs">100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [04:21&lt;00:00, 13.06s/it]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">sns.heatmap(<br>    score_matrix.mean(-<span class="hljs-number">1</span>),<br>    cmap=<span class="hljs-string">&#x27;viridis&#x27;</span>,<br>    xticklabels=[<span class="hljs-string">&quot;&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> G_list],<br>    yticklabels=[<span class="hljs-string">&quot;&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> C_list],<br>)<br>plt.xlabel(<span class="hljs-string">r&quot;$\gamma$&quot;</span>)<br>plt.ylabel(<span class="hljs-string">r&quot;$C$&quot;</span>)<br>plt.title(<span class="hljs-string">r&quot;Test accuracy under different $C$ and $\gamma$&quot;</span>)<br>plt.show()<br>argmax = score_matrix.mean(-<span class="hljs-number">1</span>).argmax()<br>argmax_row, argmax_col = argmax // <span class="hljs-built_in">len</span>(G_list), argmax % <span class="hljs-built_in">len</span>(G_list)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best parameters : C=&#123;:.4f&#125;, γ=&#123;:.4f&#125;, acc=&#123;:.2f&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(<br>    C_list[argmax_row],<br>    G_list[argmax_col],<br>    score_matrix.mean(-<span class="hljs-number">1</span>).flatten()[argmax] * <span class="hljs-number">100</span>,<br>))<br></code></pre></td></tr></table></figure><img src="/2023/05/08/SVM%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/output_16_0.png" class=""><pre><code class="hljs">Best parameters : C=6.9519, γ=1.1288, acc=98.11%</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MyFirstBlog</title>
    <link href="/2023/05/08/MyFirstBlog/"/>
    <url>/2023/05/08/MyFirstBlog/</url>
    
    <content type="html"><![CDATA[<h1 id="Hexo写博客实验总结"><a href="#Hexo写博客实验总结" class="headerlink" title="Hexo写博客实验总结"></a>Hexo写博客实验总结</h1><h2 id="博客主题及其选取原因"><a href="#博客主题及其选取原因" class="headerlink" title="博客主题及其选取原因"></a>博客主题及其选取原因</h2><p>在实验中，我选择了Hexo官方推荐的Fluid主题作为我的博客主题。选取该主题的原因如下：</p><ul><li>该主题采用了现代化的设计语言和风格，视觉效果非常优美和清新。</li><li>该主题提供了丰富的页面和组件，如文章列表、标签云、友情链接等，使得博客的展示更加丰富和多样化。</li><li>该主题支持响应式设计和移动端优化，可以在不同设备上提供一致的用户体验。</li></ul><h2 id="博客页面布局及其设计思路"><a href="#博客页面布局及其设计思路" class="headerlink" title="博客页面布局及其设计思路"></a>博客页面布局及其设计思路</h2><p>我的博客页面布局如下：</p><ul><li>顶部导航栏：包含了博客的Logo、主页链接、归档链接、分类链接和标签链接。</li><li>主体内容区域：显示博客文章列表或者文章内容。</li><li>侧边栏：包含了标签云、分类列表、最近文章、友情链接等组件。</li></ul><p>该布局的设计思路是：将导航栏置于页面的顶部，方便用户快速访问博客的主要功能；将主体内容区域放置在页面的中心位置，突出博客的核心内容；将侧边栏放置在页面的侧边，提供一些额外的信息和功能，增加博客的互动性和可读性。</p><h2 id="博客样式设计及其美学考量"><a href="#博客样式设计及其美学考量" class="headerlink" title="博客样式设计及其美学考量"></a>博客样式设计及其美学考量</h2><p>在我的博客样式设计中，我遵循了以下美学考量：</p><ul><li>简洁：尽可能去掉不必要的装饰和干扰，突出博客的核心内容。</li><li>清晰：使用简单的排版和明确的标题，方便用户快速浏览和理解文章。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/05/08/hello-world/"/>
    <url>/2023/05/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
